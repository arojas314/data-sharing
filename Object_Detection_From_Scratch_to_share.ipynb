{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsjDCIat6_UK"
   },
   "source": [
    "# Image Classification and Object Localization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpiJj8ym0v0-"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AoilhmYe1b5t",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, re, time, json\n",
    "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\n",
    "import numpy as np\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "# import tensorflow_datasets as tfds\n",
    "\n",
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "os.environ['KMP_DUPLICATION_LIB_OK']=\"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmoFKEd98MP3"
   },
   "source": [
    "# Visualization Utilities\n",
    "\n",
    "These functions are used to draw bounding boxes around the digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tBjj1Fg-i_lc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USx9tRBF8hWy"
   },
   "source": [
    "These utilities are used to visualize the data and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cCpkS9C_H7Tl",
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1 # Gobal batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data\n",
    "Read images into an array of (N, H, W, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from PIL import Image\n",
    "# Read training and testing dataset\n",
    "def get_images_labels_bboxes(fdir):\n",
    "    \"\"\"\n",
    "    Returns: (training_images, training_labels, training_bboxs\n",
    "    \"\"\"\n",
    "    \n",
    "    # read in training imagery as (N x H x W x D)\n",
    "    # fdir = r\"E:\\projects\\burner-airfield\\data\\processed\\gt\\train\"\n",
    "    img_fps = glob.glob(os.path.join(fdir, \"*.tif\"))\n",
    "    # loop through each img, get matching label file, add to list and combine all imgs into array of (N  H x W x D)\n",
    "    img_list = []\n",
    "    labels_coords = [] # list of tuples (xmin, ymin, xmax, ymax)\n",
    "    labels_class = [] # list of labels classes (in same order as labels_coords)\n",
    "    for img_fp in img_fps:\n",
    "        label_fp = img_fp.replace(\".tif\",\".json\")\n",
    "        # Get image as array\n",
    "        img_np = np.array(Image.open(img_fp).convert(\"RGB\"))\n",
    "        img_list.append(img_np)\n",
    "        with open(label_fp) as f:\n",
    "            img_label = json.load(f)\n",
    "            for label in img_label['shapes']:\n",
    "                # get bbox coords (label[\"points\"][0] is upper left coords, label[\"points\"][1] is lower right coords)\n",
    "                xmin,ymin = label['points'][0][0], label[\"points\"][0][1]\n",
    "                xmax,ymax = label['points'][1][0], label[\"points\"][1][1]\n",
    "                labels_coords.append([xmin,ymin,xmax,ymax])\n",
    "                if label['label']==\"GCP_RED\":\n",
    "                    lbl = 1\n",
    "                elif label['label']==\"GCP\":\n",
    "                    lbl = 2\n",
    "                labels_class.append(lbl)\n",
    "    # img_stack = np.stack(img_list, axis=0)\n",
    "    img_stack = np.array(img_list)\n",
    "    \n",
    "    return (img_stack, np.array(labels_class), np.array(labels_coords)) # is rehsape necessary for labels_class?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_images, training_labels, training_bboxes) = get_images_labels_bboxes(\"./train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_images, test_labels, test_bboxes) = get_images_labels_bboxes(\"./test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZE8dgyPC1_6m"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fXo6GuvL3EB"
   },
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from PIL import Image\n",
    "# Read training and testing dataset\n",
    "def get_images_labels_bboxes(fdir):\n",
    "    \"\"\"\n",
    "    Returns: (training_images, training_labels, training_bboxs\n",
    "    \"\"\"\n",
    "    \n",
    "    # read in training imagery as (N x H x W x D)\n",
    "    # fdir = r\"E:\\projects\\burner-airfield\\data\\processed\\gt\\train\"\n",
    "    img_fps = glob.glob(os.path.join(fdir, \"*.tif\"))\n",
    "    # loop through each img, get matching label file, add to list and combine all imgs into array of (N  H x W x D)\n",
    "    img_list = []\n",
    "    labels_coords = [] # list of tuples (xmin, ymin, xmax, ymax)\n",
    "    labels_class = [] # list of labels classes (in same order as labels_coords)\n",
    "    for img_fp in img_fps:\n",
    "        label_fp = img_fp.replace(\".tif\",\".json\")\n",
    "        # Get image as array\n",
    "        img_np = np.array(Image.open(img_fp).convert(\"RGB\"))\n",
    "        img_list.append(img_np)\n",
    "        with open(label_fp) as f:\n",
    "            img_label = json.load(f)\n",
    "            for label in img_label['shapes']:\n",
    "                # get bbox coords (label[\"points\"][0] is upper left coords, label[\"points\"][1] is lower right coords)\n",
    "                xmin,ymin = label['points'][0][0], label[\"points\"][0][1]\n",
    "                xmax,ymax = label['points'][1][0], label[\"points\"][1][1]\n",
    "                labels_coords.append([xmin,ymin,xmax,ymax])\n",
    "                if label['label']==\"GCP_RED\":\n",
    "                    lbl = 1\n",
    "                elif label['label']==\"GCP\":\n",
    "                    lbl = 2\n",
    "                labels_class.append(lbl)\n",
    "    # img_stack = np.stack(img_list, axis=0)\n",
    "    img_stack = np.array(img_list)\n",
    "    \n",
    "    return (img_stack, np.array(labels_class), np.array(labels_coords)) # is rehsape necessary for labels_class?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(training_images, training_labels, training_bboxes) = get_images_labels_bboxes(\"/mnt/e/projects/burner-airfield/data/processed/gt/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(test_images, test_labels, test_bboxes) = get_images_labels_bboxes(\"/mnt/e/projects/burner-airfield/data/processed/gt/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yZ4tjPKvL2eh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8nHWWkS_eeZ"
   },
   "source": [
    "## Define the Network\n",
    "\n",
    "Here, you'll define your custom CNN.\n",
    "- `feature_extractor`: these convolutional layers extract the features of the image.\n",
    "- `classifier`:  This defines the output layer that predicts among 2 categories (GCP_RED and GCP (or 1 and 2))\n",
    "- `bounding_box_regression`: This defines the output layer that predicts 4 numeric values, which define the coordinates of the bounding box (xmin, ymin, xmax, ymax)\n",
    "- `final_model`: This combines the layers for feature extraction, classification and bounding box prediction.  \n",
    "  - This is a branching model, because the model splits to produce two kinds of output (a category and set of numbers).  \n",
    "- `define_and_compile_model`: choose the optimizer and metrics, then compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56y8UNFQIVwj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Feature extractor is the CNN that is made up of convolution and pooling layers.\n",
    "'''\n",
    "def feature_extractor(inputs):\n",
    "    x = tf.keras.layers.Conv2D(16, activation='relu', kernel_size=3, input_shape=(424, 424, 3))(inputs)\n",
    "    x = tf.keras.layers.AveragePooling2D((2, 2))(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(32,kernel_size=3,activation='relu')(x)\n",
    "    x = tf.keras.layers.AveragePooling2D((2, 2))(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(64,kernel_size=3,activation='relu')(x)\n",
    "    x = tf.keras.layers.AveragePooling2D((2, 2))(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "'''\n",
    "dense_layers adds a flatten and dense layer.\n",
    "This will follow the feature extraction layers\n",
    "'''\n",
    "def dense_layers(inputs):\n",
    "    x = tf.keras.layers.Flatten()(inputs)\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "'''\n",
    "Classifier defines the classification output.\n",
    "This has a set of fully connected layers and a softmax layer.\n",
    "'''\n",
    "def classifier(inputs):\n",
    "    classification_output = tf.keras.layers.Dense(2, activation='softmax', name = 'classification')(inputs)\n",
    "    # classification_output = tf.keras.layers.Dense(1, activation='softmax', name = 'classification')(inputs) # works but so should above\n",
    "    return classification_output\n",
    "\n",
    "\n",
    "'''\n",
    "This function defines the regression output for bounding box prediction.\n",
    "Note that we have four outputs corresponding to (xmin, ymin, xmax, ymax)\n",
    "'''\n",
    "def bounding_box_regression(inputs):\n",
    "    bounding_box_regression_output = tf.keras.layers.Dense(units = '4', name = 'bounding_box')(inputs)\n",
    "    return bounding_box_regression_output\n",
    "\n",
    "\n",
    "def final_model(inputs):\n",
    "    feature_cnn = feature_extractor(inputs)\n",
    "    dense_output = dense_layers(feature_cnn)\n",
    "\n",
    "    '''\n",
    "    The model branches here.\n",
    "    The dense layer's output gets fed into two branches:\n",
    "    classification_output and bounding_box_output\n",
    "    '''\n",
    "    classification_output = classifier(dense_output)\n",
    "    bounding_box_output = bounding_box_regression(dense_output)\n",
    "\n",
    "    model = tf.keras.Model(inputs = inputs, outputs = [classification_output, bounding_box_output])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def define_and_compile_model(inputs):\n",
    "    model = final_model(inputs)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss = {'classification' : 'sparse_categorical_crossentropy',\n",
    "                          'bounding_box' : 'mse'\n",
    "                         },\n",
    "                  metrics = {'classification' : 'accuracy',\n",
    "                             'bounding_box' : 'mse'\n",
    "                            })\n",
    "    # Returns full and compiled model\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(424, 424, 3))\n",
    "# inputs = tf.keras.layers.Input(shape=(None, 424, 424, 3))\n",
    "model = define_and_compile_model(inputs)\n",
    "\n",
    "# print model layers\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CuhDh8ao8VyB"
   },
   "source": [
    "### Train and validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combine train bbox and train labels into dictionary\n",
    "trainTargets = {\n",
    "    \"classification\": training_labels,\n",
    "    \"bounding_box\": training_bboxes\n",
    "}\n",
    "\n",
    "testTargets = {\n",
    "    \"classification\": test_labels,\n",
    "    \"bounding_box\": test_bboxes\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TTwH_P-ZJ_xx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 5 # 45\n",
    "steps_per_epoch = trainTargets[\"bounding_box\"].shape[0]//BATCH_SIZE  # need number of training items only!\n",
    "validation_steps = 1\n",
    "\n",
    "history = model.fit(training_images, trainTargets,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    validation_data=(test_images, testTargets),\n",
    "                    validation_steps=validation_steps, epochs=EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate!\n",
    "loss, classification_loss, bounding_box_loss, classification_accuracy, bounding_box_mse = model.evaluate(test_images, testTargets, steps=1)\n",
    "print(\"Validation accuracy: \", classification_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "C3_W1_Lab_3_Object_Localization.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
